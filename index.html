<!DOCTYPE HTML>
<html lang="en">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7c0afb0b6b2261a569d12c2527f382b4";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhenqi He</title>

    <meta name="author" content="Zhenqi He">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zhenqi He
                </p>
                <p>
		I am a first-year PhD student at the Hong Kong University of Science and Technology (HKUST), supervised by Prof. Long Chen.
    Before that, I obtained my M.S. and BSc degree from the University of Hong Kong, where I was advised by <a href="https://www.kaihan.org/">Prof. Kai HAN</a> in Visual AI Lab.  

    I am fortunate to have internships at Huawei Hong Kong Research Center and Hong Kong Observatory.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zhenqi_he@connect.hku.hk">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=mz-ccasAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/he_zhenqi">X</a> &nbsp;/&nbsp;
                  <a href="https://www.researchgate.net/profile/Zhenqi-He">ResearchGate</a> &nbsp;/&nbsp;
                  <!-- <a href="https://github.com/jonbarron/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests include Open-World Learning, Generative Models and AI4Science.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/hypCD.jpg' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2504.06120?">
                  <span class="papertitle">Hyperbolic Category Discovery</span>
                </a>
                <br>
                Yuanpei Liu*, <strong>Zhenqi He*</strong>, Kai Han (*: Equal Contribution);
                <br>
                <em>CVPR</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2504.06120?" target="_blank">[Paper]</a> / <a href="https://visual-ai.github.io/hypcd/" target="_blank">[Code]</a>
                <p></p>
                <p>
                  We introduce HypCD, a simple Hyperbolic framework for learning hierarchy-aware representations and classifiers for generalized Category Discovery. HypCD first transforms the Euclidean embedding space of the backbone network into hyperbolic space, facilitating subsequent representation and classification learning by considering both hyperbolic distance and the angle between samples. 
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/TransNuSeg_Model.png' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2307.08051">
                  <span class="papertitle">TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation</span>
                </a>
                <br>
                <strong>Zhenqi He</strong>, Mathias Unberath, Jing Ke, Yiqing Shen;
                <br>
                <em>MICCAI</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2307.08051" target="_blank">[Paper]</a> / <a href="https://github.com/zhenqi-he/transnuseg" target="_blank">[Code]</a>
                <p></p>
                <p>
                  This paper proposes a lightweight multi-task framework for nuclei segmentation, namely TransNuSeg, as the first attempt at an entirely Swin-Transformer driven architecture. Innovatively, to alleviate the prediction inconsistency between branches, we propose a self-distillation loss that regulates the consistency between the nuclei decoder and normal edge decoder. And an innovative attention-sharing scheme that shares attention heads amongst all decoders is employed to leverage the high correlation between tasks.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/artifusion.png' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2307.14262">
                  <span class="papertitle">Artifact Restoration in Histology Images with Diffusion Probabilistic Models</span>
                </a>
                <br>
                <strong>Zhenqi He</strong>, Junjun He, Jin Ye, Yiqing Shen;
                <br>
                <em>MICCAI</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2307.14262" target="_blank">[Paper]</a> / <a href="https://github.com/zhenqi-he/ArtiFusion" target="_blank">[Code]</a>
                <p></p>
                <p>
                  This is the first attempt at a denoising diffusion probabilistic model for histological artifact restoration, called ArtiFusion. Specifically, ArtiFusion formulates the artifact region restoration as a gradual denoising process, and its training relies solely on artifact-free images to simplify the training complexity. Furthermore, to capture local-global correlations in the regional artifact restoration, a novel Swin-Transformer denoising architecture is designed, along with a time token scheme. Our extensive evaluations demonstrate the effectiveness of ArtiFusion as a pre-processing method for histology analysis, which can successfully preserve the tissue structures and stain style in artifact-free regions during the restoration.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/coling.jpg' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2409.11056">
                  <span class="papertitle">Large Language Models are Good Multi-lingual Learners: When LLMs Meet Cross-lingual Prompts</span>
                </a>
                <br>
                Teng Wang, <strong>Zhenqi He</strong>,Wing-Yin Yu, Xiaojin Fu, Xiongwei Han;
                <br>
                <em>COLING</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2409.11056" target="_blank">[Paper]</a> 
                <p></p>
                <p>
                  We introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-toMIP instances.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/AIME.png' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-66535-6_9">
                  <span class="papertitle">Histology Image Artifact Restoration with Lightweight Transformer and Diffusion Model</span>
                </a>
                <br>
                Chong Wang, <strong>Zhenqi He</strong>, Junjun He, Jin Ye, Yiqing Shen;
                <br>
                <em>AIME</em>, 2024
                <br>
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-66535-6_9" target="_blank">[Paper]</a> / <a href="https://github.com/zhenqi-he/artifact-restoration" target="_blank">[Code]</a>
                <p></p>
                <p>
                  In this paper, we propose a lightweight transformer based framework for histological artifacts restoration. In comparison to existing generative adversarial network (GAN) based solutions, our method minimizes changes in morphology while maximizing preservation of the stain style during the restoration of the artifact. By providing a more reliable and accurate restoration of artifact-affected areas, our model facilitates better analysis and interpretation of histological images, thereby potentially improving the accuracy of tumor diagnosis and treatment decisions.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='images/LatentArtifusion.png' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2407.20172">
                  <span class="papertitle">Latent ArtiFusion: a Effective and Efficient Histological Artifacts Restoration Framework</span>
                </a>
                <br>
                <strong>Zhenqi He</strong>, Wenrui Liu, Minghao Yin, Kai Han;
                <br>
                <em>DGM4MICCAI</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2407.20172" target="_blank">[Paper]</a> / <a href="https://github.com/bugs-creator/LatentArtiFusion" target="_blank">[Code]</a>
                <p></p>
                <p>
                  In this paper, we propose a novel framework, namely LatentArtiFusion, which leverages the latent diffusion model (LDM) to reconstruct histological artifacts with high performance and computational efficiency. Unlike traditional pixel-level diffusion frameworks, LatentArtiFusion executes the restoration process in a lower-dimensional latent space, significantly improving computational efficiency. Through extensive experiments on real-world histology datasets, LatentArtiFusion demonstrates remarkable speed, outperforming state-of-the-art pixel-level diffusion frameworks by more than 30×.
                </p>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a>Reviewer, MICCAI 2025</a>
                <br>
                <a>Reviewer, MICCAI 2024</a>
                <br>
                <a>Reviewer, DGM4MICCAI 2024</a>
              </td>
            </tr>
						
						
        
            
          
        </td>
      </tr>
    </table>

    <h2>Education</h2>

    <table>
      <tr>
        <td style="padding:10px; width:100px; vertical-align:top;">
          <img src="images/hkust.png" width="80px">
        </td>
        <td style="padding:10px; vertical-align:top;">
          <strong>Ph.D. in Computer Science and Engineering</strong><br>
          <a href="https://cse.hkust.edu.hk/">HKUST</a>, Clear Water Bay<br>
          2025 - 2029<br>
          Advisor: <a href="https://cse.hkust.edu.hk/~lchen/">Prof. Long Chen</a>
        </td>
      </tr>
    
      <tr>
        <td style="padding:10px; width:100px; vertical-align:top;">
          <img src="images/hku.png" width="80px">
        </td>
        <td style="padding:10px; vertical-align:top;">
          <strong>MSc. in Artificial Intelligence</strong><br>
          <a >The University of Hong Kong</a>, Hong Kong<br>
          Sep. 2023 - Jan. 2025<br>
          Advisor: <a href="https://www.kaihan.org/">Prof. Kai Han</a>
          <br>
          Grade: Distinction
        </td>
      </tr>
    
      <tr>
        <td style="padding:10px; width:100px; vertical-align:top;">
          <img src="images/hku.png" width="80px">
        </td>
        <td style="padding:10px; vertical-align:top;">
          <strong>BSc in Mathematics (Double Major in Computer Science)</strong><br>
          <a>The University of Hong Kong</a>, Hong Kong<br>
          Sep. 2018 - Jun. 2023
        </td>
      </tr>
    </table>
    
    <h2>Experiences</h2>

<table>
  <tr>
    <td style="padding:10px; width:100px; vertical-align:top;">
      <img src="images/huawei.jpeg" width="80px">
    </td>
    <td style="padding:10px; vertical-align:top;">
      <strong>Research Intern</strong><br>
      <a>Huawei Hong Kong Research Center</a>, Hong Kong<br>
      Jan. 2024 – Dec. 2024<br>
      Mentor: <a href="https://sunchumin.weebly.com/">Dr. Sun Chumin</a>
    </td>
  </tr>

  <tr>
    <td style="padding:10px; width:100px; vertical-align:top;">
      <img src="images/hko.png" width="80px">
    </td>
    <td style="padding:10px; vertical-align:top;">
      <strong>Research Intern</strong><br>
      <a>Hong Kong Observatory</a>, Hong Kong<br>
      Jan. 2022 – Jan. 2023
    </td>
  </tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:center;font-size:small;">
        This page is build on <a href="https://github.com/jonbarron/jonbarron_website">source code</a>
      </p>
    </td>
  </tr>
</tbody></table>
</td>
</tr>
</table>



  </body>
</html>
